{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from box_utils import nms, calibrate_box, get_image_boxes, convert_to_square, _preprocess\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTCNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MTCNN1](image/MTCNN1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MTCNN2](image/MTCNN2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, c, h, w].\n",
    "        Returns:\n",
    "            a float tensor with shape [batch_size, c*h*w].\n",
    "        \"\"\"\n",
    "\n",
    "        # without this pretrained model isn't working\n",
    "        x = x.transpose(3, 2).contiguous()\n",
    "\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNet, self).__init__()\n",
    "        # suppose we have input with size HxW, then\n",
    "        # after first layer: H - 2,\n",
    "        # after pool: ceil((H - 2)/2),\n",
    "        # after second conv: ceil((H - 2)/2) - 2,\n",
    "        # after last conv: ceil((H - 2)/2) - 4,\n",
    "        # and the same for W\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, ??, ??, 1)),\n",
    "            ('prelu1', nn.PReLU(??)),\n",
    "            ('pool1', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(??, ??, ??, 1)),\n",
    "            ('prelu2', nn.PReLU(??)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(??, ??, ??, 1)),\n",
    "            ('prelu3', nn.PReLU(??))\n",
    "        ]))\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n",
    "\n",
    "        weights = np.load('weights/pnet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            b: a float tensor with shape [batch_size, 4, h', w'].\n",
    "            a: a float tensor with shape [batch_size, 2, h', w'].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv4_1(x)\n",
    "        b = self.conv4_2(x) # Bounding Box Regression\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNet, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, ??, ??, 1)),\n",
    "            ('prelu1', nn.PReLU(??)),\n",
    "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(??, ??, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(??)),\n",
    "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(??, ??, 2, 1)),\n",
    "            ('prelu3', nn.PReLU(64)),\n",
    "\n",
    "            ('flatten', Flatten()),\n",
    "            ('conv4', nn.Linear(576, 128)),\n",
    "            ('prelu4', nn.PReLU(128))\n",
    "        ]))\n",
    "\n",
    "        self.conv5_1 = nn.Linear(128, 2)\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "\n",
    "        weights = np.load('weights/rnet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            b: a float tensor with shape [batch_size, 4].\n",
    "            a: a float tensor with shape [batch_size, 2].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv5_1(x) \n",
    "        b = self.conv5_2(x) # Bounding Box Regression\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ONet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 32, 3, 1)),\n",
    "            ('prelu1', nn.PReLU(32)),\n",
    "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(32, 64, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(64)),\n",
    "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(64, 64, 3, 1)),\n",
    "            ('prelu3', nn.PReLU(64)),\n",
    "            ('pool3', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv4', nn.Conv2d(64, 128, 2, 1)),\n",
    "            ('prelu4', nn.PReLU(128)),\n",
    "\n",
    "            ('flatten', Flatten()),\n",
    "            ('conv5', nn.Linear(1152, 256)),\n",
    "            ('drop5', nn.Dropout(0.25)),\n",
    "            ('prelu5', nn.PReLU(256)),\n",
    "        ]))\n",
    "\n",
    "        self.conv6_1 = nn.Linear(256, ??)\n",
    "        self.conv6_2 = nn.Linear(256, ??)\n",
    "        self.conv6_3 = nn.Linear(256, ??)\n",
    "\n",
    "        weights = np.load('weights/onet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            c: a float tensor with shape [batch_size, 10].\n",
    "            b: a float tensor with shape [batch_size, 4].\n",
    "            a: a float tensor with shape [batch_size, 2].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv6_1(x)\n",
    "        b = self.conv6_2(x) # Bounding Box Regression\n",
    "        c = self.conv6_3(x) # Face Landmark Detection\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return c, b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run P-Net, Generate Bounding boxes and do NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_first_stage(image, net, scale, threshold):\n",
    "    \"\"\"Run P-Net, generate bounding boxes, and do NMS.\n",
    "\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        net: an instance of pytorch's nn.Module, P-Net.\n",
    "        scale: a float number,\n",
    "            scale width and height of the image by this number.\n",
    "        threshold: a float number,\n",
    "            threshold on the probability of a face when generating\n",
    "            bounding boxes from predictions of the net.\n",
    "\n",
    "    Returns:\n",
    "        a float numpy array of shape [n_boxes, 9],\n",
    "            bounding boxes with scores and offsets (4 + 1 + 4).\n",
    "    \"\"\"\n",
    "\n",
    "    # scale the image and convert it to a float array\n",
    "    width, height = image.size\n",
    "    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\n",
    "    img = image.resize((sw, sh), Image.BILINEAR)\n",
    "    img = np.asarray(img, 'float32')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = Variable(torch.FloatTensor(_preprocess(img)))\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            net = net.cuda()\n",
    "    output = net(img)\n",
    "    probs = output[1].data.cpu().numpy()[0, 1, :, :]\n",
    "    offsets = output[0].data.cpu().numpy()\n",
    "    # probs: probability of a face at each sliding window\n",
    "    # offsets: transformations to true bounding boxes\n",
    "\n",
    "    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n",
    "    return boxes[keep]\n",
    "\n",
    "\n",
    "def _generate_bboxes(probs, offsets, scale, threshold):\n",
    "    \"\"\"Generate bounding boxes at places\n",
    "    where there is probably a face.\n",
    "\n",
    "    Arguments:\n",
    "        probs: a float numpy array of shape [n, m].\n",
    "        offsets: a float numpy array of shape [1, 4, n, m].\n",
    "        scale: a float number,\n",
    "            width and height of the image were scaled by this number.\n",
    "        threshold: a float number.\n",
    "\n",
    "    Returns:\n",
    "        a float numpy array of shape [n_boxes, 9]\n",
    "    \"\"\"\n",
    "\n",
    "    # applying P-Net is equivalent, in some sense, to\n",
    "    # moving 12x12 window with stride 2\n",
    "    stride = 2\n",
    "    cell_size = 12\n",
    "\n",
    "    # indices of boxes where there is probably a face\n",
    "    inds = np.where(probs > threshold)\n",
    "\n",
    "    if inds[0].size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # transformations of bounding boxes\n",
    "    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n",
    "    # they are defined as:\n",
    "    # w = x2 - x1 + 1\n",
    "    # h = y2 - y1 + 1\n",
    "    # x1_true = x1 + tx1*w\n",
    "    # x2_true = x2 + tx2*w\n",
    "    # y1_true = y1 + ty1*h\n",
    "    # y2_true = y2 + ty2*h\n",
    "\n",
    "    offsets = np.array([tx1, ty1, tx2, ty2])\n",
    "    score = probs[inds[0], inds[1]]\n",
    "\n",
    "    # P-Net is applied to scaled images\n",
    "    # so we need to rescale bounding boxes back\n",
    "    bounding_boxes = np.vstack([\n",
    "        np.round((stride*inds[1] + 1.0)/scale),\n",
    "        np.round((stride*inds[0] + 1.0)/scale),\n",
    "        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\n",
    "        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\n",
    "        score, offsets\n",
    "    ])\n",
    "    # why one is added?\n",
    "\n",
    "    return bounding_boxes.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image, min_face_size=20.0,\n",
    "                 thresholds=[0.6, 0.7, 0.8],\n",
    "                 nms_thresholds=[0.7, 0.7, 0.7]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        min_face_size: a float number.\n",
    "        thresholds: a list of length 3.\n",
    "        nms_thresholds: a list of length 3.\n",
    "\n",
    "    Returns:\n",
    "        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n",
    "        bounding boxes and facial landmarks.\n",
    "    \"\"\"\n",
    "\n",
    "    # LOAD MODELS\n",
    "    pnet = PNet()\n",
    "    rnet = RNet()\n",
    "    onet = ONet()\n",
    "    onet.eval()\n",
    "\n",
    "    # BUILD AN IMAGE PYRAMID\n",
    "    width, height = image.size\n",
    "    min_length = min(height, width)\n",
    "\n",
    "    min_detection_size = 12\n",
    "    factor = 0.707  # sqrt(0.5)\n",
    "\n",
    "    # scales for scaling the image\n",
    "    scales = []\n",
    "\n",
    "    # scales the image so that\n",
    "    # minimum size that we can detect equals to\n",
    "    # minimum face size that we want to detect\n",
    "    m = min_detection_size/min_face_size\n",
    "    min_length *= m\n",
    "\n",
    "    factor_count = 0\n",
    "    # scales = [0.6, 0.42, 0.30, 0.21, 0.15, 0.10, 0.07, 0.05, 0.03]\n",
    "    while min_length > min_detection_size:\n",
    "        scales.append(m*factor**factor_count)\n",
    "        min_length *= factor\n",
    "        factor_count += 1\n",
    "\n",
    "    # STAGE 1\n",
    "\n",
    "    # it will be returned\n",
    "    bounding_boxes = []\n",
    "\n",
    "    # run P-Net on different scales\n",
    "    for s in scales:\n",
    "        boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n",
    "        bounding_boxes.append(boxes)\n",
    "\n",
    "    # collect boxes (and offsets, and scores) from different scales\n",
    "    bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "    bounding_boxes = np.vstack(bounding_boxes)\n",
    "\n",
    "    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0]) # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "\n",
    "    # use offsets predicted by pnet to transform bounding boxes\n",
    "    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "    # shape [n_boxes, 5]\n",
    "\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 2\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n",
    "    with torch.no_grad():\n",
    "        img_boxes = Variable(torch.FloatTensor(img_boxes))\n",
    "        if torch.cuda.is_available():\n",
    "            rnet = rnet.cuda()\n",
    "            img_boxes = img_boxes.cuda()\n",
    "    output = rnet(img_boxes)\n",
    "    offsets = output[0].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[1].data.cpu().numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[1])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[1]) # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 3\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n",
    "    if len(img_boxes) == 0:\n",
    "        return [], []\n",
    "    with torch.no_grad():\n",
    "        img_boxes = Variable(torch.FloatTensor(img_boxes))\n",
    "        if torch.cuda.is_available():\n",
    "            onet = onet.cuda()\n",
    "            img_boxes = img_boxes.cuda()\n",
    "    output = onet(img_boxes)\n",
    "    landmarks = output[0].data.cpu().numpy()  # shape [n_boxes, 10]\n",
    "    offsets = output[1].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[2].data.cpu().numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[2])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    # compute landmark points\n",
    "    width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n",
    "    height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n",
    "    xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n",
    "    landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n",
    "    landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n",
    "\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets)\n",
    "    keep = nms(bounding_boxes, nms_thresholds[2], mode='min') # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    return bounding_boxes, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\n",
    "    \"\"\"Draw bounding boxes and facial landmarks.\n",
    "\n",
    "    Arguments:\n",
    "        img: an instance of PIL.Image.\n",
    "        bounding_boxes: a float numpy array of shape [n, 5].\n",
    "        facial_landmarks: a float numpy array of shape [n, 10].\n",
    "\n",
    "    Returns:\n",
    "        an instance of PIL.Image.\n",
    "    \"\"\"\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    draw = ImageDraw.Draw(img_copy)\n",
    "\n",
    "    # Draw Bounding boxes\n",
    "    for b in bounding_boxes:\n",
    "        draw.rectangle([\n",
    "            (b[0], b[1]), (b[2], b[3])\n",
    "        ], outline='white')\n",
    "\n",
    "    # Draw Facial Landmarks\n",
    "    for p in facial_landmarks:\n",
    "        for i in range(5):\n",
    "            draw.ellipse([\n",
    "                (p[i] - 1.0, p[i + 5] - 1.0),\n",
    "                (p[i] + 1.0, p[i + 5] + 1.0)\n",
    "            ], outline='blue')\n",
    "\n",
    "    return img_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/office1.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/office2.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/office3.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/office4.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/office5.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/image1.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/image2.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/image3.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('image/image4.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1596544264958"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}